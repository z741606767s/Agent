import hashlib
from contextlib import asynccontextmanager
import re
import os
import tempfile
import requests  # æ­£ç¡®å¯¼å…¥requestsåº“

import redis
from openai import OpenAI
import asyncio
import edge_tts
import replicate
import uvicorn
from dotenv import load_dotenv
from moviepy.video.VideoClip import ImageClip, TextClip
from moviepy.audio.io.AudioFileClip import AudioFileClip
from moviepy.audio.AudioClip import CompositeAudioClip
from moviepy.audio.fx.all import SilentAudioClip
from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip
from fastapi import FastAPI, Request, Depends
from fastapi.responses import JSONResponse

# =============== é…ç½®ä½ çš„ Key ===============
load_dotenv()
api_key = os.getenv("OPENAPI_KEY")
base_url = os.getenv("OPENAPI_BASE_URL")
client = OpenAI(
    base_url=base_url,
    api_key=api_key
)

# è®¾ç½®ç›®å½•
AUDIO_DIR = os.path.join(os.getcwd(), "generated_audio")
TEMP_DIR = tempfile.gettempdir()  # ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
os.makedirs(AUDIO_DIR, exist_ok=True)


# =============== å¯åŠ¨fastapi ===============
@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        app.state.redis_client = redis.asyncio.Redis.from_url(
            'redis://10.32.120.2:56379/2',
            encoding='utf-8',
            decode_responses=True,
            password='foobared'
        )
        # æµ‹è¯•è¿æ¥
        await app.state.redis_client.ping()
        print("Redis è¿æ¥æˆåŠŸ")
    except redis.exceptions.AuthenticationError as e:
        print(f"Redis è®¤è¯å¤±è´¥: {e}")
    except Exception as e:
        print(f"Redis è¿æ¥é”™è¯¯: {e}")

    yield
    if hasattr(app.state, 'redis_client'):
        await app.state.redis_client.close()


app = FastAPI(lifespan=lifespan)


# =============== chatæ¥å£ ===============
@app.post("/chat")
async def chat(request: Request):
    data = await request.json()
    prompt = data.get("prompt")
    if not prompt:
        return JSONResponse(content={"error": "No prompt provided"}, status_code=400)

    try:
        response = await create_video_from_text(prompt)
        return JSONResponse(content={"response": {"video": response}})
    except Exception as e:
        print(f"ç”Ÿæˆè§†é¢‘æ—¶å‡ºé”™: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


# =============== 1. ç”Ÿæˆç»“æ„åŒ–å‰§æœ¬ï¼ˆå«åœºæ™¯æè¿°ï¼‰ ===============
async def generate_script_with_scenes(prompt):
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    key = f"video:{prompt_hash}"

    cached_response = await app.state.redis_client.get(key)
    if cached_response:
        return cached_response

    try:
        response = client.chat.completions.create(
            model="deepseek-r1",
            messages=[
                {"role": "system",
                 "content": "ä½ æ˜¯ä¸€ä¸ªç¼–å‰§ã€‚è¯·å°†è¾“å…¥æ‰©å±•ä¸ºä¸€ä¸ª30ç§’å†…çš„çŸ­å‰§ï¼ŒåŒ…å«å¤šä¸ªåœºæ™¯ã€‚æ¯ä¸ªåœºæ™¯éœ€æœ‰ç”»é¢æè¿°å’Œå¯¹è¯ã€‚è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\nåœºæ™¯1ï¼šåœºæ™¯æè¿°\nè§’è‰²1ï¼šå¯¹è¯å†…å®¹\nåœºæ™¯2ï¼šåœºæ™¯æè¿°\nè§’è‰²2ï¼šå¯¹è¯å†…å®¹\n...\nä¸è¦ä½¿ç”¨Markdownæ ¼å¼ï¼Œä¸è¦ä½¿ç”¨é¡¹ç›®ç¬¦å·(*)ï¼Œç›´æ¥ä½¿ç”¨çº¯æ–‡æœ¬ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )

        print(f"ç”Ÿæˆå‰§æœ¬: {response.choices[0].message.content}")

        if response.choices[0].message.content:
            await app.state.redis_client.set(key, response.choices[0].message.content, ex=60 * 60 * 24)

        return response.choices[0].message.content
    except Exception as e:
        print(f"å‰§æœ¬ç”Ÿæˆå¤±è´¥: {e}")
        return None


# =============== 2. æ–‡ç”Ÿå›¾ï¼šç”¨ Stable Diffusion ç”Ÿæˆåœºæ™¯å›¾ ===============
async def generate_image(prompt, output_file):
    print(f"æ­£åœ¨ç”Ÿæˆç”»é¢: {prompt}")
    try:
        # æ›´æ–°ä¸ºæœ‰æ•ˆçš„Stable Diffusionæ¨¡å‹ç‰ˆæœ¬
        output = replicate.run(
            "stability-ai/stable-diffusion:27b93a2413e7f36cd83da926f365628f9f90697fdbe47dbaeefe9d417197",
            input={"prompt": prompt, "width": 1280, "height": 720}
        )
        image_url = output[0]

        # ä¸‹è½½å›¾ç‰‡
        response = requests.get(image_url)
        if response.status_code != 200:
            raise Exception(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")

        with open(output_file, 'wb') as f:
            f.write(response.content)
            print(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸï¼Œä¿å­˜ä¸º: {output_file}")
            return image_url
    except Exception as e:
        print(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªå ä½å›¾ç‰‡
        from PIL import Image, ImageDraw
        try:
            img = Image.new('RGB', (1280, 720), color=(73, 109, 137))
            draw = ImageDraw.Draw(img)
            draw.text((640, 360), f"åœºæ™¯: {prompt[:30]}...", fill='white', anchor='mm')
            img.save(output_file)
            return output_file
        except Exception as pil_err:
            print(f"åˆ›å»ºå ä½å›¾ç‰‡å¤±è´¥: {pil_err}")
            raise


# =============== 3. æ–‡å­—è½¬è¯­éŸ³ ===============
VOICES = {
    "å°æ˜": "zh-CN-XiaoyiNeural",
    "å°é¸Ÿ": "zh-CN-XiaoxiaoNeural"
}


async def text_to_speech(text, voice, output_file):
    try:
        communicate = edge_tts.Communicate(text, voice)
        await communicate.save(output_file)
        print(f"è¯­éŸ³ç”ŸæˆæˆåŠŸ: {output_file}")
    except Exception as e:
        print(f"è¯­éŸ³ç”Ÿæˆå¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„éŸ³é¢‘æ–‡ä»¶ä½œä¸ºå ä½ç¬¦
        import wave
        try:
            with wave.open(output_file, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(44100)
                wf.writeframes(b'')
        except Exception as wave_err:
            print(f"åˆ›å»ºå ä½éŸ³é¢‘å¤±è´¥: {wave_err}")


# =============== 4. è§£æå‰§æœ¬å¹¶ç”Ÿæˆåª’ä½“ ===============
def parse_script(script):
    """è§£æå‰§æœ¬ï¼Œæå–åœºæ™¯å’Œå¯¹è¯ï¼ˆå…¼å®¹Markdownæ ¼å¼ï¼‰"""
    scenes = []
    lines = script.strip().split('\n')
    current_scene = None

    for line in lines:
        line = line.strip()
        # è·³è¿‡ç©ºè¡Œå’Œè§£é‡Šæ€§æ–‡å­—
        if not line or line.startswith(("å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ª", "**çŸ­å‰§ï¼š", "---")):
            continue

        # æ£€æµ‹åœºæ™¯è¡Œï¼ˆå…¼å®¹Markdownæ ¼å¼ï¼‰
        scene_match = re.match(r'[\*\s]*åœºæ™¯(\d+)ï¼š(.+?)[\*\s]*$', line, re.UNICODE)
        if scene_match:
            if current_scene:
                scenes.append(current_scene)
            scene_desc = scene_match.group(2).strip()
            current_scene = {"desc": scene_desc, "dialogues": []}
            continue

        # æ£€æµ‹å¯¹è¯è¡Œï¼ˆå…¼å®¹Markdownæ ¼å¼ï¼‰
        dialogue_match = re.match(r'[\*\s]*([^ï¼š]+)ï¼š(.+?)[\*\s]*$', line, re.UNICODE)
        if dialogue_match and current_scene:
            char = dialogue_match.group(1).strip()
            text = dialogue_match.group(2).strip()
            if char in VOICES:
                current_scene["dialogues"].append({"char": char, "text": text})

    # æ·»åŠ æœ€åä¸€ä¸ªåœºæ™¯
    if current_scene:
        scenes.append(current_scene)

    return scenes


async def create_video_from_text(input_text):
    # 1. ç”Ÿæˆå‰§æœ¬
    script = await generate_script_with_scenes(input_text)
    if not script:
        raise ValueError("æ— æ³•ç”Ÿæˆå‰§æœ¬")
    print("ğŸ“œ ç”Ÿæˆå‰§æœ¬ï¼š\n", script)

    # 2. è§£æå‰§æœ¬
    scenes = parse_script(script)
    print(f"è§£æå‡ºçš„åœºæ™¯: {scenes}")

    if not scenes:
        raise ValueError("æ— æ³•è§£æå‰§æœ¬ï¼Œæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåœºæ™¯")

    # 3. ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆå›¾ç‰‡
    for i, scene in enumerate(scenes):
        img_file = os.path.join(TEMP_DIR, f"scene_{i}.jpg")
        await generate_image(scene["desc"], img_file)
        scene["image"] = img_file
        # éªŒè¯å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_file) or os.path.getsize(img_file) == 0:
            raise FileNotFoundError(f"åœºæ™¯å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {img_file}")

    # 4. ç”Ÿæˆè¯­éŸ³å¹¶åˆ›å»ºè§†é¢‘ç‰‡æ®µ
    audio_clips = []
    video_clips = []
    current_time = 0

    for i, scene in enumerate(scenes):
        print(f"å¤„ç†åœºæ™¯{i}: {scene['image']}")

        # åˆ›å»ºå›¾ç‰‡å‰ªè¾‘ï¼Œæ˜ç¡®æŒ‡å®šdurationå‚æ•°
        try:
            img_clip = ImageClip(scene["image"], duration=5)  # ç›´æ¥åœ¨æ„é€ å‡½æ•°ä¸­è®¾ç½®æ—¶é•¿
        except Exception as e:
            print(f"åˆ›å»ºå›¾ç‰‡å‰ªè¾‘å¤±è´¥: {e}")
            # å°è¯•æ›¿ä»£æ–¹æ¡ˆ
            img_clip = ImageClip(scene["image"])
            img_clip = img_clip.set_duration(5)  # å¤‡é€‰æ–¹æ¡ˆ

        # ç”Ÿæˆå¯¹è¯è¯­éŸ³
        scene_audio_clips = []
        for dialogue in scene["dialogues"]:
            audio_file = os.path.join(AUDIO_DIR, f"audio_{i}_{dialogue['char']}.mp3")
            await text_to_speech(dialogue["text"], VOICES[dialogue["char"]], audio_file)

            if os.path.exists(audio_file) and os.path.getsize(audio_file) > 0:
                try:
                    audio_clip = AudioFileClip(audio_file)
                    scene_audio_clips.append(audio_clip)

                    # åˆ›å»ºå­—å¹•
                    txt_clip = TextClip(
                        f"{dialogue['char']}: {dialogue['text']}",
                        fontsize=40, color='white',
                        font='SimHei',
                        size=(1280, 100),
                        method='caption'
                    ).set_position(('center', 'bottom')).set_duration(audio_clip.duration).set_start(current_time)

                    video_clips.append(txt_clip)
                    current_time += audio_clip.duration
                except Exception as e:
                    print(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")

        # å¤„ç†åœºæ™¯æ—¶é•¿
        if not scene_audio_clips:
            scene_duration = 3  # 3ç§’çš„é™é»˜åœºæ™¯
            current_time += scene_duration
            img_clip = img_clip.set_duration(scene_duration)
        else:
            # è°ƒæ•´å›¾ç‰‡æŒç»­æ—¶é—´åŒ¹é…å¯¹è¯æ€»æ—¶é•¿
            scene_duration = sum([clip.duration for clip in scene_audio_clips])
            img_clip = img_clip.set_duration(scene_duration)

            # æ·»åŠ éŸ³é¢‘åˆ°æ€»éŸ³é¢‘åˆ—è¡¨
            for audio_clip in scene_audio_clips:
                audio_clip = audio_clip.set_start(current_time - scene_duration)
                audio_clips.append(audio_clip)

        # æ·»åŠ å›¾ç‰‡åˆ°è§†é¢‘
        img_clip = img_clip.set_start(current_time - scene_duration)
        video_clips.append(img_clip)

    # 5. åˆæˆéŸ³é¢‘
    if not audio_clips:
        final_audio = SilentAudioClip(duration=current_time)
    else:
        final_audio = CompositeAudioClip(audio_clips)

    # 6. åˆæˆè§†é¢‘
    if not video_clips:
        raise ValueError("æ²¡æœ‰ç”Ÿæˆä»»ä½•è§†é¢‘ç‰‡æ®µ")

    final_video = CompositeVideoClip(video_clips, size=(1280, 720))
    final_video = final_video.set_audio(final_audio)

    output_path = os.path.join(AUDIO_DIR, "dynamic_story_video.mp4")
    final_video.write_videofile(output_path, fps=24, codec="libx264", audio_codec="aac")

    print(f"âœ… è§†é¢‘å·²ç”Ÿæˆï¼š{output_path}")
    return output_path


# =============== è¿è¡Œ ===============
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8002)
